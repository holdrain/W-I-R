{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.6765941\n",
      "model:GN_0.25-forged_bit_acc:0.4631999731063843 acc:0.0\n",
      "score: 0.6304311\n",
      "model:GN_0.25-forged_bit_acc:0.46380001306533813 acc:0.0\n",
      "score: 0.6517542\n",
      "model:GN_0.25-forged_bit_acc:0.4616999626159668 acc:0.0\n",
      "score: 0.6943655\n",
      "model:GN_0.25-forged_bit_acc:0.46309995651245117 acc:0.0\n",
      "score: 0.7044579\n",
      "model:GN_0.25-forged_bit_acc:0.46490001678466797 acc:0.0\n",
      "score: 0.6594108\n",
      "model:GN_0.25-forged_bit_acc:0.46220001578330994 acc:0.0\n",
      "score: 0.35920802\n",
      "model:GN_0.25-forged_bit_acc:0.46220001578330994 acc:0.0\n",
      "score: 0.71154994\n",
      "model:GN_0.25-forged_bit_acc:0.46470001339912415 acc:0.0\n",
      "score: 0.62132776\n",
      "model:GN_0.25-forged_bit_acc:0.4602999687194824 acc:0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m set_seeds(seed)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_sample:\n\u001b[0;32m---> 88\u001b[0m     wm_images,residual_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[get_wmimages(text\u001b[38;5;241m=\u001b[39mt,image_i\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m),attack_num),encoder\u001b[38;5;241m=\u001b[39mencoder) \u001b[38;5;28;01mfor\u001b[39;00m i,t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_list)])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     wm_images,residual_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[get_wmimages(text\u001b[38;5;241m=\u001b[39mt,image_i\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m((i)\u001b[38;5;241m*\u001b[39mattack_num,(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mattack_num),encoder\u001b[38;5;241m=\u001b[39mencoder) \u001b[38;5;28;01mfor\u001b[39;00m i,t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_list)])\n",
      "Cell \u001b[0;32mIn[2], line 88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m set_seeds(seed)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_sample:\n\u001b[0;32m---> 88\u001b[0m     wm_images,residual_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43mget_wmimages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattack_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i,t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_list)])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     wm_images,residual_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[get_wmimages(text\u001b[38;5;241m=\u001b[39mt,image_i\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m((i)\u001b[38;5;241m*\u001b[39mattack_num,(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mattack_num),encoder\u001b[38;5;241m=\u001b[39mencoder) \u001b[38;5;28;01mfor\u001b[39;00m i,t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_list)])\n",
      "File \u001b[0;32m/data/shared/Huggingface/sharedcode/Stegastamp_CR/ipynb/functions.py:36\u001b[0m, in \u001b[0;36mget_wmimages\u001b[0;34m(ds, image_i, text, encoder, device, model_choice)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m image_i:\n\u001b[0;32m---> 36\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstega\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m             fingerprint_image,_,residual_image \u001b[38;5;241m=\u001b[39m encoder(fingerprints,image)\n",
      "File \u001b[0;32m/data/shared/Huggingface/sharedcode/Stegastamp_CR/dataset.py:24\u001b[0m, in \u001b[0;36mCustomImageFolder.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(filename)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch200/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/torch200/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch200/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "from utils.helpers import *\n",
    "from dataset import CustomImageFolder\n",
    "from torchvision import transforms\n",
    "from functools import partial\n",
    "import torch\n",
    "from functions import *\n",
    "from clustering_function import text_low,text_mid,text_high\n",
    "from torchvision.datasets import ImageFolder\n",
    "from clustering_function import cluster\n",
    "from sklearn.manifold import TSNE\n",
    "from setting import *\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# model and data setting\n",
    "mi = \"\"\n",
    "device = torch.device(\"cuda:7\")\n",
    "normalize_ = transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "un_normalize_ = transforms.Normalize([-1,-1,-1],[2,2,2])\n",
    "data_dir = {\"COCO\":MSCOCO_TEST_PATH,\"CelebA\":CELEBAHQ_VAL_PATH}\n",
    "\n",
    "# forge setting \n",
    "text_num = 4         # 1 4\n",
    "# set_seeds(2024)\n",
    "message_len = 100\n",
    "tolerant = cal_tolerant(message_len)\n",
    "batch_size = 32\n",
    "alpha = 1\n",
    "use_cluster = True\n",
    "use_vae = False\n",
    "attack_num = 30\n",
    "random_sample = True\n",
    "\n",
    "for model_choice in [\"hidden\"]:\n",
    "    for data_choice in [\"COCO\"]:\n",
    "        transform_pipe = [\n",
    "            transforms.Resize((image_resolution[data_choice],image_resolution[data_choice])),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        ckp_dir = f\"weights/{mi}weight/{data_choice}/{model_choice}\"\n",
    "        if model_choice == 'hidden':\n",
    "            transform_pipe.append(transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]))\n",
    "        transform = transforms.Compose(transform_pipe)\n",
    "        if data_choice == \"CelebA\":\n",
    "            ds = ImageFolder(data_dir[data_choice],transform=transform)\n",
    "        else:\n",
    "            ds = CustomImageFolder(data_dir[data_choice],transform,num=1000)\n",
    "\n",
    "        # functions\n",
    "        get_wmimages = partial(get_wmimages,ds=ds,device=device,model_choice=model_choice)\n",
    "        get_images = partial(get_images,ds=ds,device=device)\n",
    "        extract_message = partial(extract_message,device=device,model_choice=model_choice)\n",
    "\n",
    "        ckp_list = sorted(os.listdir(ckp_dir))\n",
    "        if f\"{mi}OO\" in ckp_list:\n",
    "            ckp_list.remove(f\"{mi}OO\")\n",
    "            ckp_list.insert(0,f\"{mi}OO\")\n",
    "\n",
    "\n",
    "        for gpname,text_strings in zip([\"mid\"],[text_mid]):\n",
    "            csv_file = f\"../forged_results/oriforge/{data_choice}_{model_choice}_{gpname}_{text_num}_{mi}_alpha{alpha}_{attack_num}.csv\"\n",
    "            text_list = [generate_message(message_len,t,1) for t in text_strings]\n",
    "            for op in ckp_list:\n",
    "                if op == \"OO\":\n",
    "                    ckp_path = os.path.join(ckp_dir,op,'epoch_499_state100.pth')\n",
    "                elif op == 'emperical':\n",
    "                    ckp_path = os.path.join(ckp_dir,op,'epoch_499_state.pth')\n",
    "                else:\n",
    "                    ckp_path = os.path.join(ckp_dir,op,'epoch_99_state.pth')\n",
    "                encoder,decoder = load_weights(ckp_path,model_choice,message_len)\n",
    "                encoder.eval(),decoder.eval()\n",
    "                target_image,target_residual = get_wmimages(text=text_list[-1],image_i=[-1],encoder=encoder)\n",
    "                # watermarked images by different text\n",
    "                bit_acc_list = []\n",
    "                acc_list = []\n",
    "                if random_sample:\n",
    "                    wm_images,residual_predictions = zip(*[get_wmimages(text=t,image_i=random.sample(range(1000),attack_num),encoder=encoder) for i,t in enumerate(text_list)])\n",
    "                else:\n",
    "                    wm_images,residual_predictions = zip(*[get_wmimages(text=t,image_i=range((i)*attack_num,(i+1)*attack_num),encoder=encoder) for i,t in enumerate(text_list)])\n",
    "                if use_vae:\n",
    "                    residual_predictions = [get_residual_prediction(wm_images_i,batch_size,device=device,method='VAE') for wm_images_i in wm_images]\n",
    "                wm_images = list(wm_images)\n",
    "                residual_predictions = list(residual_predictions)\n",
    "                residual_predictions.append(target_residual)\n",
    "\n",
    "                # # TSNE\n",
    "                if text_num != 1:\n",
    "                    data = torch.cat(residual_predictions,dim=0).detach().cpu()\n",
    "                    flattened_data = data.view((text_num)*attack_num+1,-1).numpy()\n",
    "                    tsne = TSNE(n_components=2, random_state=5000)\n",
    "                    reduced_data = tsne.fit_transform(flattened_data)\n",
    "\n",
    "                    # cluster\n",
    "                    cluster_labels,cluster_centers = cluster(reduced_data,text_num,\"km\")\n",
    "                    score = silhouette_score(reduced_data,cluster_labels)\n",
    "                    print(\"score:\",score)\n",
    "                    target_label = cluster_labels[-1]\n",
    "                    if use_cluster:\n",
    "                        attack_samples = data[cluster_labels == target_label]\n",
    "                    else:\n",
    "                        attack_samples = residual_predictions[-2]\n",
    "                        print(attack_samples.shape)\n",
    "\n",
    "                    # rescale\n",
    "                    attack_samples = un_normalize_(attack_samples)  # 0-1\n",
    "                    attack_sample = attack_samples.mean(axis=0)\n",
    "                    attack_sample = normalize_(attack_sample) # -1,1\n",
    "                    attack_pattern = attack_sample.unsqueeze(0).to(device)\n",
    "\n",
    "                else:\n",
    "                    attack_sample = residual_predictions[-1]\n",
    "                    attack_pattern = attack_sample.to(device)\n",
    "\n",
    "                # forge\n",
    "                original_images = get_images(image_i=range(900,1000)).to(device)\n",
    "                forged_image = (original_images + alpha * attack_pattern)\n",
    "                forged_image = torch.clamp(forged_image, min=-1, max=1)\n",
    "                forged_prediction,_ = extract_message(image_tensor=forged_image,decoder=decoder,model_choice=model_choice)\n",
    "                gt_text = text_list[-1].repeat(forged_prediction.size(0),1).to(device)\n",
    "                difference = (forged_prediction != gt_text).float()\n",
    "                correct = (difference.sum(dim=1)<=tolerant).float()\n",
    "                acc = correct.mean(dim=0).item()\n",
    "                bitwise_accuracy = (1.0 - difference.mean(dim=1))\n",
    "                bitwise_accuracy = torch.mean(bitwise_accuracy)\n",
    "                print(f\"model:{op}-forged_bit_acc:{bitwise_accuracy.item()} acc:{acc}\")\n",
    "                csv_dict = {\"model\":op,\"bit_acc\":bitwise_accuracy.item(),\"acc\":acc}\n",
    "                with open(csv_file, mode='a', newline='') as file:\n",
    "                    fieldnames = csv_dict.keys()\n",
    "                    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                    if file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "                    writer.writerow(csv_dict)\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_csv(file1, file2, output_file,score):\n",
    "    # 读取两个CSV文件\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    if score == 'forgedacc' or score == 'xyscore':\n",
    "        score = 'bit_acc'\n",
    "    # 去掉第二个文件中的`mi`前缀\n",
    "    df2['model'] = df2['model'].str.replace('mi', '', regex=False)\n",
    "    \n",
    "    # 提取 `bit_acc` 列并重命名\n",
    "    bit_acc1 = df1[['model', score]].rename(columns={score: 'wo_bit_acc'})\n",
    "    bit_acc2 = df2[['model', score]].rename(columns={score: 'w_bit_acc'})\n",
    "    \n",
    "    # 合并数据\n",
    "    merged_df = pd.merge(bit_acc1, bit_acc2, on='model', how='left')\n",
    "    \n",
    "    # 如果在第二个文件中没有出现的模型，其 w_bit_acc 设为 wo_bit_acc\n",
    "    merged_df['w_bit_acc'].fillna(merged_df['wo_bit_acc'], inplace=True)\n",
    "    \n",
    "    # 格式化结果\n",
    "    merged_df['wo_bit_acc'] = merged_df['wo_bit_acc'].apply(lambda x: f\"{x:.4f}\")\n",
    "    merged_df['w_bit_acc'] = merged_df['w_bit_acc'].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    # 保存到新的CSV文件\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "\n",
    "score = \"xyscore\"\n",
    "for dataset in [\"CelebA\"]:\n",
    "    for model in [\"stega\"]:\n",
    "        if score == \"forgedacc\":\n",
    "            csv_file1 = f\"/data/shared/Huggingface/sharedcode/Stegastamp_CR/forged_results/{dataset}_{model}_mid_4__alpha1_30.csv\"\n",
    "            csv_file2 = f\"/data/shared/Huggingface/sharedcode/Stegastamp_CR/forged_results/{dataset}_{model}_mid_4_mi_alpha1_30.csv\"\n",
    "            output_file = f'/data/shared/Huggingface/sharedcode/Stegastamp_CR/forged_results/comparsion_{dataset}_{model}_{score}.csv'\n",
    "        elif score == \"score\":\n",
    "            csv_file1 = f\"/data/shared/Huggingface/sharedcode/Stegastamp_CR/KM_results/score/{dataset}_{model}_rand_4_.csv\"\n",
    "            csv_file2 = f\"/data/shared/Huggingface/sharedcode/Stegastamp_CR/KM_results/score/{dataset}_{model}_rand_4_mi.csv\"\n",
    "            output_file = f'/data/shared/Huggingface/sharedcode/Stegastamp_CR/KM_results/score/comparsion_{dataset}_{model}_{score}.csv'\n",
    "        else:\n",
    "            csv_file1 = f\"/data/shared/Huggingface/sharedcode/Stegastamp_CR/xyattackresults/{dataset}_{model}__1_3.csv\"\n",
    "            csv_file2 = f\"/data/shared/Huggingface/sharedcode/Stegastamp_CR/xyattackresults/{dataset}_{model}_mi_1_3.csv\"\n",
    "            output_file = f'/data/shared/Huggingface/sharedcode/Stegastamp_CR/xyattackresults/comparsion_{dataset}_{model}_{score}_1_3.csv'\n",
    "        process_csv(csv_file1, csv_file2, output_file,score)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
